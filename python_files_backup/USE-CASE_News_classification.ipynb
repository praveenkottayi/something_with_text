{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective : Identify topics or tags/labels from news articles and map to prospective sales vertical in a prescriptive way.Â¶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author : \n",
    "Praveen Vijayan ,\n",
    "TCS A&I - Pune"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dataset used # AG's News Topic Classification Dataset\n",
    "# Version 3, Updated 09/09/2015\n",
    "\n",
    "AG is a collection of more than 1 million news articles. News articles have been gathered from more than 2000  \n",
    "news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine \n",
    "which has been running since July, 2004. The dataset is provided by the academic comunity for research purposes\n",
    "in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression,\n",
    "data streaming, and any other non-commercial activity.The AG's news topic classification dataset is constructed by choosing 4 largest classes from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Here we have taken random samples from subset of the Business class from the above dataset.\n",
    "The four subsets are :\n",
    "1. Accounting\n",
    "2. Aero ( Aircraft Industry)\n",
    "3. BFSI\n",
    "4. Oil & gas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "import csv\n",
    "#import nltk\n",
    "import string\n",
    "import re\n",
    "import sys\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_news_category(x) :\n",
    "    class_output_NB = clf_NB.predict(x)\n",
    "    class_description = \" \"\n",
    "    if class_output_NB == '1':\n",
    "        class_description = \"accounting\"\n",
    "    if class_output_NB == '2':\n",
    "        class_description = \"aero\"\n",
    "    if class_output_NB == '3':\n",
    "        class_description = \"bfsi\"\n",
    "    if class_output_NB == '4':\n",
    "        class_description = \"oil&gas\"        \n",
    "    print(\"@NB : This NEWS is related to : \" + class_description)        \n",
    "    class_output_svm = clf_svm.predict(x)\n",
    "    class_description = \" \"\n",
    "    if class_output_NB == '1':\n",
    "        class_description = \"accounting\"\n",
    "    if class_output_NB == '2':\n",
    "        class_description = \"aero\"\n",
    "    if class_output_NB == '3':\n",
    "        class_description = \"bfsi\"\n",
    "    if class_output_NB == '4':\n",
    "        class_description = \"oil&gas\"       \n",
    "    print(\"@SVM : This NEWS is related to : \" + class_description)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read CSV file & load into list\n",
    "with open(\"sub_News_data_train.csv\",'r') as my_file:\n",
    "    reader = csv.reader(my_file, delimiter=',')\n",
    "    news_data_train = list(reader)\n",
    "    \n",
    "with open(\"sub_News_data_test.csv\",'r') as my_file:\n",
    "    reader = csv.reader(my_file, delimiter=',')\n",
    "    news_data_test = list(reader)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'Nortel shares down following news of RCMP probe', 'TORONTO - Shares of Nortel Networks dipped on the TSX in the wake of news the RCMP has launched an investigation into accounting practices at the telecommunications company. ']\n"
     ]
    }
   ],
   "source": [
    "print(news_data_train[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class label : 1\n"
     ]
    }
   ],
   "source": [
    "print(\"class label : \" + news_data_train[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heading : Nortel shares down following news of RCMP probe\n"
     ]
    }
   ],
   "source": [
    "print(\"Heading : \" + news_data_train[3][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content : TORONTO - Shares of Nortel Networks dipped on the TSX in the wake of news the RCMP has launched an investigation into accounting practices at the telecommunications company. \n"
     ]
    }
   ],
   "source": [
    "print(\"Content : \" +news_data_train[3][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucketing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords_file = open(\"stopwords.txt\", \"r\" , encoding=\"utf8\") \n",
    "stopwords = stopwords_file.read()\n",
    "#stopwords.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STOPWORD_customized = {'will','new','york','will' ,'quot','year ','company ','week', 'one','two','three','reuters','reuters' , 'monday','tuesday','wednesday','thursday','friday','saturday','sunday','yesterday'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = [] # class\n",
    "text_doc = [] # news text \n",
    "c =0 # to control count of \n",
    "for i in news_data_train:\n",
    "    c= c+1\n",
    "    if c < 50000:\n",
    "        label.append(i[0])            \n",
    "        merged = i[1]+i[2] # combining header and content \n",
    "        lowers = merged.lower()   # to lower      \n",
    "        no_punctuation = re.sub(r'[^\\w\\s]',' ', lowers)    # remove punctuation\n",
    "        temp = ' '.join(word for word in no_punctuation.split() if len(word)>3) # remove samll words\n",
    "        filtered = ' '.join(word for word in temp.split() if word not in stopwords.split()) # remove stopwords\n",
    "        filtered = ' '.join(word for word in filtered.split() if word not in STOPWORD_customized) # remove stopwords\n",
    "        text_doc.append(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "with codecs.open(\"parsed_NLP_text.txt\", 'w', encoding='utf_8') as f: \n",
    "    for sent in text_doc:\n",
    "        f.write(sent +'. '+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "investors nortel jobsottawa nortel networks corp investors predicted telecom equipment giant slash jobs reports long overdue results shrugged news criminal probe high profile accounting woes\n"
     ]
    }
   ],
   "source": [
    "print(text_doc[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(text_doc)\n",
    "text = \"\"\n",
    "for i in range (1,len(text_doc)):\n",
    "    temp = (text_doc[i])\n",
    "    text = text + temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_count = Counter()\n",
    "word_count.update((word for word in text.split()))\n",
    "#for word, count in word_count.most_common():\n",
    "    #print(word, \":\" ,count,\"\\n\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'utf-8'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getdefaultencoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#Counter(label).keys() # equals to list(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Counter(label).values() # counts the elements' frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2637, 7482)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "corpus = text_doc  # processed text corpous \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#tf_transformer = TfidfTransformer(use_idf=True).fit(X)\n",
    "#X_tf = tf_transformer.transform(X)\n",
    "#X_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2637x3701 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 48012 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfvectorizer = TfidfVectorizer(max_df=0.5, max_features= 10000, min_df=2, stop_words='english',use_idf= True)\n",
    "X_tfidf = tfvectorizer.fit_transform(corpus)\n",
    "X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification (NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X -> full corpus\n",
    "# X_tfidf -> TFIDF reduced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X  has text data as martrix\n",
    "y = label\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_NB = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "clf = MultinomialNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_NB.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.99      0.99      0.99       463\n",
      "          2       0.97      0.99      0.98       222\n",
      "          3       0.99      0.98      0.98       619\n",
      "          4       0.99      1.00      1.00      1333\n",
      "\n",
      "avg / total       0.99      0.99      0.99      2637\n",
      "\n",
      "[[ 460    1    1    1]\n",
      " [   0  220    1    1]\n",
      " [   4    6  604    5]\n",
      " [   1    0    2 1330]]\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "expected = label\n",
    "predicted = clf_NB.predict(X)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_NB.fit(X_tfidf, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.98      0.97      0.98       463\n",
      "          2       0.99      0.86      0.92       222\n",
      "          3       0.95      0.95      0.95       619\n",
      "          4       0.97      1.00      0.98      1333\n",
      "\n",
      "avg / total       0.97      0.97      0.97      2637\n",
      "\n",
      "[[ 449    1    5    8]\n",
      " [   1  190   24    7]\n",
      " [   5    1  585   28]\n",
      " [   1    0    1 1331]]\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "expected = label\n",
    "predicted = clf_NB.predict(X_tfidf)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification (svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf_svm = svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
    "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "  tol=0.001, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00       463\n",
      "          2       1.00      1.00      1.00       222\n",
      "          3       1.00      1.00      1.00       619\n",
      "          4       1.00      1.00      1.00      1333\n",
      "\n",
      "avg / total       1.00      1.00      1.00      2637\n",
      "\n",
      "[[ 463    0    0    0]\n",
      " [   0  222    0    0]\n",
      " [   0    0  619    0]\n",
      " [   0    0    0 1333]]\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "expected = label\n",
    "predicted = clf_svm.predict(X)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm.fit(X_tfidf, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#clf.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       1.00      1.00      1.00       463\n",
      "          2       1.00      1.00      1.00       222\n",
      "          3       1.00      1.00      1.00       619\n",
      "          4       1.00      1.00      1.00      1333\n",
      "\n",
      "avg / total       1.00      1.00      1.00      2637\n",
      "\n",
      "[[ 463    0    0    0]\n",
      " [   0  222    0    0]\n",
      " [   0    0  618    1]\n",
      " [   1    0    0 1332]]\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "expected = label\n",
    "predicted = clf_svm.predict(X_tfidf)\n",
    "# summarize the fit of the model\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy on training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random_news_generator = random.randint(1,len(news_data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Continuing to snap up branch networks in states with large Hispanic populations, Citigroup Inc. Tuesday, Aug. 24, agreed to buy First American Bank of Texas for an undisclosed price.'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_data_test[random_news_generator][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_text_doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = news_data_test[random_news_generator][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#new_text = [\"AFP - The world economy, powered in part by China, will grow this year at its briskest pace in almost three decades -- five percent -- before losing steam in 2005 in the face of higher oil prices, the IMF said.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text_doc.append(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tfvectorizer.transform(new_text_doc).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@NB : This NEWS is related to : bfsi\n",
      "@SVM : This NEWS is related to : bfsi\n"
     ]
    }
   ],
   "source": [
    "find_news_category(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
